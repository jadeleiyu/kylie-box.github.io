---
title:  "Fisher's LDA"
categories: posts
mathjax: true
---

## Intro
This blog post is about the Fisher's LDA. LDA is a bit ambiguous, don't confuse with the Latent Dirichilet Allocation, another famous generative model can be used in documents topic classfication. The idea of Fisher's LDA is that we would like to classify the data points $$ x\in \mathbb{R} $$ into k classes $$ y\in \mathbb{N}_k $$. This is a generative model for classification. That is, we can write the out the joint distribution of $$ x, y $$. 

$$ p(x,y; \theta) = p(x|y; \theta) \cdot p(y; \theta)$$

We assume

$$ x|y_i; \theta \sim \mathcal{N}(X | \mu_i, \Sigma) $$

where $$ \Sigma $$ is shared across all classes. $$ \theta = (\{\mu_i\}_{i=1}^k, \Sigma, \pi )$$, $$ \mu_i $$ is the mean of class $$ i $$, $$ \pi = P(y=1) $$ We can use the joint MLE to estimate $$ \theta $$.

We assume that the 


$$ \hat{\theta} = argmax_{\theta \in \Theta} \sum_{i=1}^n logp(x_i|y_i; \theta) $$

### Invariance property of MLE
	
***Theorem*** If $$ \hat{\theta} $$ is the MLE of the parameter, and $$ g $$ is a function, then $$ g(\hat{\theta}) $$ is the MLE of $$ g(\theta) $$.

Thus given $$ g(x) = x^{-1}$$, $$ \hat{g(\theta)} $$ is the MLE of $$ g(\theta) $$