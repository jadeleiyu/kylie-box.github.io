---
title:  "Sampling Methods for Training Word Embeddings"
categories: posts
mathjax: true
bibliography: bibliography.bib
---

## Abstract
Pre-trained word embeddings are ubiquitous in the field of natural language processing. As an extended work of Hilbert-MLE~\citep{hilbert}, this work investigates sampling-based implementations of training word embeddings which improve the scalability and efficiency of the models. The main contributions of this work are: (1)We verify the sampling-based implementations of Hilbert-MLE are on par with the dense model; (2)We validate the efficiency and scalability of sampling-based methods.

## Background and Motivation
One of the most fundamental elements in NLP, word embedding is the study of mapping words in natural languages into vector representations. Since words occurring in the same context tend to have similar meanings~\citep{wittgenstein1953philosophical}, good word representations demonstrate clustering in high dimensional vector spaces. The groundbreaking work of \citet{mikolov2013distributed} represents words in low rank continuous vector spaces by introducing skip-gram negative sampling(SGNS), a fast training algorithm with simple architecture that outputs high quality word embeddings capturing semantic and syntactic features of words. SGNS~\citep{mikolov2013distributed} learns word representations through a shallow neural network by predicting the surrounding context given a center word. This work inspired many following word embedding algorithms such as GloVe~\citep{pennington-etal-2014-glove}, Swivel~\citep{shazeer2016swivel} and LDS~\citep{Arora_2016}. Meanwhile, \citet{Levy:2014:NWE:2969033.2969070} analyze the mechanism of SGNS by showing that SGNS is essentially factorizing the PMI matrix of the corpus. On the basis of these previous works, \citet{hilbert} propose a novel unified matrix factorization framework - Hilbert-MLE model, and prove its efficacy through the \textbf{dense implementation}, an algorithm that explicitly factorizes the PMI matrix. PMI is a metrics measuring the association of a pair of words. The Hilbert-MLE framework constructs dual vector representations of a word by maximizing the likelihood of co-occurrence statistics of the corpus. To parameterize the co-occurrence statistics, \citet{hilbert} introduce the notion of vector and covector representations of a word, where the vector representation captures the meaning and other linguistic features, while the covector representation demonstrates its association or affinity to another word. Concretely speaking, word ``purr`` appears very often in the context of ``cat``such that the vector of ``cat`` learns to have high association with the vector of ``purr``.